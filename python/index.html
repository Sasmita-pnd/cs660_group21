<!DOCTYPE html>
<html>
<head>

<title>Predict AQI</title>
</head>
<body bgcolor="HoneyDew">

<h1>Predict AQI</h1>
<p> Sasmita Pandey<br>CS660<br></p>
<hr>
<hr>
<BLOCKQUOTE>
<h3>AIR QUALITY INDEX</h3>

<p>The Air Quality Index (AQI) is used for reporting daily air quality. It tells us how clean or polluted air is.

</p>

<p>
The AQI is calculated based on the average concentration of a particular pollutant measured over a standard time interval (24 hours for most pollutants, 8 hours for carbon monoxide and ozone).<br> For example, the AQI for PM2.5 is based on 24-hour average concentration 
<img src="thequint-fit_2019-11_53ec129b-dcb9-4f51-abb1-e474883cb277_Capture.jpg"  width="300" height="500" Align=right>

</p>

<h3>How AQI is influenced by Climatic Conditions.</h3>
<p>Climate change influences air as ambient  air pollutants  are very sensitive to meteorological 
conditions.
</p>
<p>
Air quality is strongly dependent on weather and is therefore sensitive to climate change.<br> Recent studies have provided estimates of this climate effect through correlations of air quality with meteorological variables.

</p>
<p>How is air pollution affected by weather depends-- some types of pollution are worse in the summer heat, while others are worse in cold winter weather. <br>The same atmospheric conditions that create weather-- air pressure, temperature, and humidity-- also affect air quality.

</p>
<p>
Because of climate warming, the Earth experiences more extreme weather, such as heat waves and drought, which can negatively Impact air quality.
</p>


<h3>Purpose/Idea</h3>
<p>
The Purpose of this project is to use Meteorological variables for past  years data for a  and use them to predict the Air Quality Index  of a particular pollutant of a Particular region.

</p>

<h3>Data Sources</h3>
<p>
To predict the air quality index of a particular region , we need the <br>

pollutant concentration of a particular pollutant (PM 2.5  AQI) which will be available in the cpcb.nic.in website,

</p>
<p>

Historical Metereological Data which will  be collected from website en.tutiempo.net/

</p>
<h3>Techniques </h3>
  
<p>Feature  engineering </p>
<p>
Supervised Learning Algorithms  
(Linear Regression
Lasso and Ridge Regression
Decision Tree Regressor
KNN Regressor
Random Forest Regressor)
</p>
<h3>Targets </h3>
  
<p>BY  MIDWAY
 Collection of Data,
Preprocess the Data

Apply Different  Supervised  algorithms and Identify  which works  best
</p>
<p>
AFTER  MIDWAY
Suggest  possible  improvements
Report Writing 
</p>
<h3>Expected Results</h3>
<p>The  model should be capable for successfully predicting the air quality index of a total county or any state or any bounded region provided with the Metereological Data 
</p>
<h3>Relevant papers</h3>
  
<p>Effect of climate change on air quality Daniel J. Jacob a,*, Darrell A. Winner b a School of Engineering and Applied Science, Harvard University, Cambridge, MA, USA bOffice of Research & Development, U.S. Environmental Protection Agency, Washington, DC, USA<br>
 <a href="https://dash.harvard.edu/handle/1/3553961">link</a>

</p>
<p>Climate Change is Threatening Air Quality across the Country
     Published: July 30th, 2019
      Research brief by Climate Central<br>
	 <a href= "https://www.climatecentral.org/news/climate-change-is-threatening-air-quality-across-the-country-2019">link</a>


</p>
<p>RESEARCH ARTICLE
    Impacts of climate change on future air quality and human health in China

</p>
	
<hr>
<h3>Analysis of  related papers..</h3>
<p>
<b>Effect of climate change on air quality Daniel J. Jacob a,*, Darrell A. Winner b a School of Engineering and Applied Science, Harvard University, Cambridge, MA, USA bOffice of Research & Development, U.S. Environmental Protection Agency, Washington, DC, USA<br>
 <a href="https://dash.harvard.edu/handle/1/3553961">link</a></b>
</p>
<p>It reviewed current knowledge of the effect of climate change
on air quality with focus on 21st-century projections for ozone and
particulate matter (PM).It studied observed correlations of
ozone and PM with meteorological variables.
The two air pollutants of most concern for public health are
surface ozone and particulate matter, and they are the focus of the
review.
It was observed that correlations of PM concentrations with meteorological variables are weaker than for ozone 
</p>

</p>
<p>
<b>Climate Change is Threatening Air Quality across the Country<br>
     Published: July 30th, 2019<br>
      Research brief by Climate Central<br>
https://www.climatecentral.org/news/climate-change-is-threatening-air-quality-across-the-country-2019</b>

</p>
<p> Climate Central analyzed summer high temperatures and used data from the NOAA/NCEI Air Stagnation Index, which incorporates upper atmospheric winds, surface winds, and precipitation to identify the number of stagnant days in a month.
 <br>The same atmospheric conditions that create weather-- air pressure, temperature, and humidity-- also affect air quality.
The analysis showed a positive correlation between summer maximum temperatures and the number of summer stagnant days.
 As the climate warms, stagnant days are projected to increase further, with up to 40 more days per year by late-century.  
And stagnation impacts air quality. When the air is stagnant, pollutants react together in the heat and sun, and high concentrations of ground-level ozone can build up. 
</p>
<p>
<b>RESEARCH ARTICLE<br>
    Impacts of climate change on future air quality and human health in China</b>
<p>
 This paper presents correlation analyses between changes in ozone and PM2.5 concentrations and these meteorological variables.
 that the increase in ozone concentrations is generally associated with the increase in temperature and the decrease in precipitation, while the increase in PM2.5 concentrations is generally associated with the decrease in boundary layer height and wind speed.
</p>


<h3>Creating Dataset</h3>
<p>
The  Pollutant data was available from 2018 onwards and  downloaded from cpcb website
The meteorological variables were webscrapped  from https://en.tutiempo.net/
To collect the meteorological variables data Html file of website was webscrapped .
Then Using beautiful soup html data was written into a csv Format<br>
<img src="csv.png"Align=center><br>
</p>
<p>
Data  was cleaned (removing null values, removing empty rows)
And  AQI data was combined to form final dataset.<br><br>
<img src="fcsv.png"Align=center><br>




</p>

<h3>Visualising Data</h3>
<p>
Dataset was first imported , and checked for null values in Dataframe, and remove the null values.<br>
<img src="visualise.png"Align=center>
<img src="nulld.png"Align=center>
<img src="visualise null.png"Align=center>
</p>
<p>
Divide dataset into dependent and independent features<br>

<img src="visualise divide data.png"Align=center>
</p>
<p>
Pairwise relationships in the dataset were visualised . The pairplot function was used to creates a grid of Axes such that each variable in data will by shared in the y-axis across a single row and in the x-axis across a single column. 
And pairwise correlation of all columns in the dataframe found using  df.corr()<br>
<img src="pairplot.png"Align=center>


</p>
<h3>Feature Importance </h3>
  
<p>Feature importance refers to a class of techniques for assigning scores to input features to a predictive model that indicates the relative importance of each feature when making a prediction.
Feature importance scores can be calculated for problems that involve predicting a numerical value, called regression, and those problems that involve predicting a class label, called classification.
 The higher the score more important or relevant is the feature towards your output variable.
Feature importance is an inbuilt class that comes with Tree Based Regressor I used Extra Tree Regressor for extracting required features for the dataset.<br>
<img src="Feature Sel.png"Align=center><br>
<img src="Feature.png"Align=center><br>
<img src="featureimp.png"Align=center>

 </p>
<p>

</p>
<h3>Linear Regression </h3>
  
<p>Since  the dataset contains more than one independent feature thus it uses multiple linear regression. 
</p>
<p>
Build the  Linear regression model<br>
<img src="lin model.png"Align=center>

</p>
<p>
Predict  Dependent variables(2.5 AQI)<br>
<img src="lin predict y.png"Align=center>
</p>
<p>
Evaluate  model by finding R-Squared value  is also known as the Coefficient of Determination. The value of R-Squared ranges from 0 to 1.The higher the R-Squared value of a model, the better is the model fitting on the data. 
However, if the R-Squared value is very close to 1, then there is a possibility of model overfitting, which should be avoided. A good model should have an R-Squared above 0.8.<br>
<img src="coef r2.png"Align=center><br>
We get R-Squared value close to 7



</p>


<h3>Ridge Regression</h3>
<p>Steps 
Build the  Linear regression model
     We generally use a scorer object with the scoring parameter in the cross_val_score function. All scorer objects follow the convention that higher return values are better than lower return values.
Thus metrics which measure the distance between the model and the data, like metrics.mean_squared_error, are available as neg_mean_squared_error which return the negated value of the metric.
 here we have mse=-2760.392573715005<br>
 <img src="ridgel.png"Align=center>
</p>
<p>
Try  if we could make our model better by hyper tuning it using Ridge Regression.
in Ridege Regression we have taken the alpha(Î»-parameter) value as a bunch of possible random values. We are doing GridSearch to find the best parameters which will work for the Ridge Regressor.
Here also the value for the scoring parameter will be neg_mean_squared_error which measures the  distance between 
      the model and the  data.
   The formula for Ridge Regression is:<br>
   <img src="ridge formula.png"Align=center><br>
   <img src="ridge.png"Align=center><br>
   <img src="Ridge param.png"Align=center><br>


</p>

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vQyoW5bFbPTNEoxzNOPXs9Y-unfgQL7nB-3HvkScimu6sJlj5BDWN2SlAgDb0iiMQ/embed?start=false&loop=false&delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
<hr/>
	
	
<hr>

<h3>Lasso Regression</h3>
<p>
Steps
</p>
<p>Build the Linear regression model.
</p>
<p>We use a scorer object with the scoring parameter in the cross_val_score function. 
All scorer objects follow the convention that higher return values are better than lower return values.
 Thus metrics which measure the distance between the model and the data, like metrics.mean_squared_error, are available as neg_mean_squared_error which return the negated value of the metric. 
Here we have mse=-2760.392573715005
</p>
<p> We try to make our model better by hyper tuning it using Lasso Regression.
 in Ridege Regression we have taken the alpha(Î»-parameter) value as a bunch of possible random values.
 We are doing GridSearch to find the best parameters which will work for the Lasso Regressor.
 Here also the value for the scoring parameter will be neg_mean_squared_error which measures the distance between the model and the data
</p>
<p>
After hypertuning we get score as -2760.3925738642843.
</p>
<p>Evaluation Metrices<br>
Mean Absolute Error: 34.31529029728029<br>
Mean Square Error: 2203.686005695834<br>
Root Mean Square Error: 46.94343410633519<br>
</p>
<h3>K-Nearest Neighbour</h3>
<p>
Build the K-Nearest Neighbour model<br>


</p>
<p>
We Evaluate  model by finding R-Squared value  i.e Coefficient of Determination. <br>
We get R-Squared value for training  Data=1.0<br>
We get R-Squared value for testing  Data=0.3820085833313066<br>
</p>
<p>Evaluation Metrices<br>
Mean Absolute Error: 36.720977777777776<br>
Mean Square Error: 3100.9031288126985<br>
Root Mean Square Error: 55.68575337384508<br>
</p>


<h3>Random Forest Regressor</h3>
<p>
Random Forest Regression is a supervised learning algorithm that uses ensemble learning method for<br> 
regression. Ensemble learning method is a technique that combines predictions from multiple machine<br> 
learning algorithms to make a more accurate prediction than a single model.It is a meta estimator that<br> 
fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to<br>
 improve the predictive accuracy and control over-fitting.<br>
</p>
<p>We Evaluate  model by finding R-Squared value  i.e Coefficient of Determination.<br>
We get R-Squared value for training  Data=0.9729739075719985<br>
We get R-Squared value for testing  Data=0.7540440934165676<br>
</p>
<p>We get mean cross valdation score =0.6815096950121835<br>
We also perform a Randomized Search and get mean score =-1888.5427588856328
</p>
<p>Evaluation Metrices<br>
Mean Absolute Error: 19.898582014790176<br>
Mean Square Error: 835.6375956118613<br>
Root Mean Square Error: 28.907396901344494<br>
</p>

<h3>XG Boost Regressor</h3>
<p>XGBoost stands for âExtreme Gradient Boostingâ. XGBoost is an optimized distributed gradient boosting<br>
 library designed to be highly efficient, flexible and portable. It implements Machine Learning algorithms<br> 
 under the Gradient Boosting framework.<br>
 It provides a parallel tree boosting to solve many data science problems in a fast and accurate way.<br>
 
</p>
<p>We Evaluate  model by finding R-Squared value  i.e Coefficient of Determination.<br>
We get R-Squared value for training  Data=0.9995968643939962<br>
We get R-Squared value for testing  Data=0.7247784976953502<br>
</p>
<p>We get mean cross valdation score =0.6548830137132338<br>
We also perform a Randomized Search and get mean score =-1952.6997222489085
</p>
<p>Evaluation Metrices<br>
Mean Absolute Error: 7.676428318689741<br>
Mean Square Error: 133.2822014854206<br>
Root Mean Square Error:  11.544791097521886<br>
</p>

<hr>
<h3>Improvements</h3>
<p>From   the  Final dataset A feature was completely deleted as not enough Records for the feature were available .
<br>
 For improving the model I calculated the average of the available records of the deleted feature and used function fillna() to fill all empty cells of that feature with the calculated  average value.
<br>
 
</p>
<p>When I Re Run all algorithms with the new dataset and could find minor improvements.
<br>

<h3>Linear regression</h3>
<p>R-Squared value</p>
<p>Before Modifications</p>
<p>We get R-Squared value for training  Data=0.6819162365647814
<br>
We get R-Squared value for testing  Data=0.6938412145242896
<br></p>

<p>After Modifications
</p>
<p>We get R-Squared value for training  Data=0.6819215499319956

<br>
We get R-Squared value for testing  Data=0.6939168889550285

<br>
</p>

<p>Evaluation Metrices Before Modifications<br>
Mean Absolute Error: 34.42900945054323
<br>
Mean Square Error: 2244.444176498912
<br>
Root Mean Square Error:  47.37556518395228
<br>
</p>
<p>Evaluation Metrices After Modifications<br>
Mean Absolute Error: 34.42704370859507
<br>
Mean Square Error: 2243.8894087005046
<br>
Root Mean Square Error:  47.36970982284465
<br>
</p>

<h3>Ridge Regression</h3>
<p>Mean Cross validation Score Before Modifications =-2760.392573715005


</p>
<p>Mean Cross validation Score After Modifications =-2768.2974430691675

</p>
<p>
After Hypertuning</p>
<p>Mean Cross validation Score Before Modifications =-2760.3925737150103


</p>
<p>Mean Cross validation Score After Modifications =-2768.297443069173

</p>
<p>Evaluation Metrices Before Modifications<br>
Mean Absolute Error: 34.31529029706646
<br>
Mean Square Error: 2203.6860057132244
<br>
Root Mean Square Error:  46.94343410652042
<br>
</p>
<p>Evaluation Metrices After Modifications<br>
Mean Absolute Error: 34.307877465585314
<br>
Mean Square Error: 2202.34877434467
<br>
Root Mean Square Error:  46.929188937639545

<br>
</p>


<h3>Lasso Regression</h3>
<p>Mean Cross validation Score Before Modifications =-2760.392573715005


</p>
<p>Mean Cross validation Score After Modifications =-2760.392573715005


</p>
<p>
After Hypertuning</p>
<p>Mean Cross validation Score Before Modifications =2760.3925738642843



</p>
<p>Mean Cross validation Score After Modifications =-2768.2974432297224


</p>
<p>Evaluation Metrices Before Modifications<br>
Mean Absolute Error: 34.31529029728029
<br>
Mean Square Error:2203.686005695834
<br>
Root Mean Square Error: 46.94343410633519

<br>
</p>
<p>Evaluation Metrices After Modifications<br>
Mean Absolute Error: 34.30787746579477
<br>
Mean Square Error: 2202.348774324336
<br>
Root Mean Square Error:  46.9291889374229


<br>
</p>

<h3>K-Nearest Neighbour</h3>
<p>R-Squared value</p>
<p>Before Modifications</p>
<p>We get R-Squared value for training  Data=1.0
<br>
We get R-Squared value for testing  Data=0.3820085833313066

<br></p>

<p>After Modifications
</p>
<p>We get R-Squared value for training  Data=1.0

<br>
We get R-Squared value for testing  Data=0.39331751605005005

<br>
</p>
<p>
After Hypertuning</p>
<p>Mean Cross validation Score Before Modifications =0.21544843109832443

</p>
<p>Mean Cross validation Score After Modifications =0.21707626193332777

</p>
<p>Evaluation Metrices Before Modifications<br>
Mean Absolute Error: 36.720977777777776

<br>
Mean Square Error:3100.9031288126985
<br>
Root Mean Square Error: 55.68575337384508

<br>
</p>
<p>Evaluation Metrices After Modifications<br>
Mean Absolute Error: 36.69868571428571
<br>
Mean Square Error:3084.7406758730162
<br>
Root Mean Square Error: 55.540441804805766



<br>
</p>




<h3>Results</h3>
<p>After trying all these  Supervised algorithms we can say that Xg Boost and  Random Forest Regressor  
give the  best performance.</p>
<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vQ99_GmA7olidO9nzxcRXrjc-L1oMorgCvSl-wMR5AXVeeV8FA2DzAP-vpqyweDjpDCgAF1Bw5APpBH/embed?start=false&loop=false&delayms=3000" frameborder="0" width="960" height="749" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
<hr>	

</BLOCKQUOTE>
</body>
</html>
